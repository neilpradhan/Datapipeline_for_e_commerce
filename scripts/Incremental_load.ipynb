{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Load\n",
    "\n",
    "For the Initial Load we will use Transform_into_data_model.ipynb, but for subsequent incremental loads we will use this file\n",
    "\n",
    "Assuming new up coming data will be filled in new_inventory_df and new_orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Note the upcoming data will be filled in new_inventory_df and new_orders_df, here I am only\n",
    "# taking the existing data for implemntation purposes\n",
    "\n",
    "new_inventory_df = pd.read_csv('../processed_data/cleaned_data/cleaned_inventory.csv')\n",
    "new_orders_df = pd.read_csv('../processed_data/cleaned_data/cleaned_orders.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for Appending to Existing Orders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the service account key file\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Dataset and table names\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Dema'\n",
    "orders_table_id = f'{project_id}.{dataset_id}.Orders'\n",
    "inventory_table_id = f'{project_id}.{dataset_id}.Inventory'\n",
    "\n",
    "\n",
    "\n",
    "# Convert dateTime column to datetime type\n",
    "new_orders_df['dateTime'] = pd.to_datetime(new_orders_df['dateTime'])\n",
    "\n",
    "# Replace NaN values in 'campaign' column with None\n",
    "new_orders_df['campaign'] = new_orders_df['campaign'].replace({pd.NA: None, float('nan'): None})\n",
    "\n",
    "# Define schema for Orders table\n",
    "orders_schema = [\n",
    "    bigquery.SchemaField(\"orderId\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"productId\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"currency\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"INT64\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"shippingCost\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"amount\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"channel\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"channelGroup\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"campaign\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"dateTime\", \"TIMESTAMP\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "# Load data into Orders table\n",
    "job_config = bigquery.LoadJobConfig(schema=orders_schema, write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "job = client.load_table_from_dataframe(new_orders_df, orders_table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete.\n",
    "print(\"Incremental load for Orders table completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to do Incremental load for dimension table Inventory we take following steps\n",
    "\n",
    "1. Do an outer join of Upcoming Source dataset with existing in Dataware house Target dataset\n",
    "     \n",
    "2. Determining rows for Update, Insert and Delete Operations using Hash mapping and outer join \n",
    "\n",
    "- equal hashes means NO CHANGE\n",
    "- Left_only means DELETE in target\n",
    "- right_only means INSERT\n",
    "- both_ with unequal hashes means UPDATE\n",
    "\n",
    "3. Pushing the changes to dataware house or Google Big Query in our case (Make sure the Inventory Surrogate key is updated appropriately)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to the service account key file\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Dataset and table names\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Dema'\n",
    "inventory_table_id = f'{project_id}.{dataset_id}.Inventory'\n",
    "\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "slowly_changing_cols = [\"name\", \"category\", \"subCategory\"]\n",
    "\n",
    "# Function to strip whitespaces\n",
    "def strip_whitespaces(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df\n",
    "\n",
    "# Function to calculate hash using hashlib\n",
    "def calculate_hash(df, cols):\n",
    "    return df[cols].astype(str).apply(lambda row: hashlib.md5(''.join(row).encode()).hexdigest(), axis=1)\n",
    "\n",
    "\n",
    "def remove_suffixes(df):\n",
    "    df.columns = df.columns.str.replace('_target', '').str.replace('_source', '')\n",
    "    return df\n",
    "\n",
    "# Load the target inventory data from BigQuery\n",
    "target_df = client.query(f\"SELECT * FROM `{inventory_table_id}`\").to_dataframe()\n",
    "\n",
    "# Rename inventory_skey column to include suffix\n",
    "target_df = target_df.rename(columns={'inventory_skey': 'inventory_skey_target'})\n",
    "\n",
    "# Strip whitespaces from columns in target_df\n",
    "target_df = strip_whitespaces(target_df, slowly_changing_cols)\n",
    "\n",
    "# Add SCD Type 2 columns to new inventory DataFrame\n",
    "new_inventory_df['startDate'] = datetime.now().strftime(DATE_FORMAT)\n",
    "new_inventory_df['endDate'] = None\n",
    "new_inventory_df['isCurrent'] = True\n",
    "\n",
    "# Strip whitespaces from columns in new_inventory_df\n",
    "new_inventory_df = strip_whitespaces(new_inventory_df, slowly_changing_cols)\n",
    "\n",
    "# Calculate hash for slowly changing columns\n",
    "target_df['hash_target'] = calculate_hash(target_df, slowly_changing_cols)\n",
    "new_inventory_df['hash_source'] = calculate_hash(new_inventory_df, slowly_changing_cols)\n",
    "\n",
    "# Determine action: NOCHANGE, DELETE, INSERT, UPDATE\n",
    "merged_df = pd.merge(target_df, new_inventory_df, on='productId', suffixes=('_target', '_source'), how='outer', indicator=True)\n",
    "merged_df['Action'] = 'NOCHANGE'\n",
    "merged_df.loc[merged_df['_merge'] == 'left_only', 'Action'] = 'DELETE'\n",
    "merged_df.loc[merged_df['_merge'] == 'right_only', 'Action'] = 'INSERT'\n",
    "merged_df.loc[(merged_df['_merge'] == 'both') & (merged_df['hash_target'] != merged_df['hash_source']), 'Action'] = 'UPDATE'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# merged_df = merged_df.drop(columns=['hash'])\n",
    "\n",
    "# # # Define the columns to keep from the merged DataFrame\n",
    "target_columns = ['productId', 'name_target', 'category_target', 'subCategory_target', 'quantity_target', 'startDate_target', 'endDate_target', 'isCurrent_target', 'inventory_skey_target']\n",
    "source_columns = ['productId', 'name_source', 'category_source', 'subCategory_source', 'quantity_source', 'startDate_source', 'endDate_source', 'isCurrent_source','inventory_skey_target']\n",
    "\n",
    "# Process unchanged records\n",
    "unchanged_records = merged_df[merged_df['Action'] == 'NOCHANGE'][target_columns]\n",
    "\n",
    "# Rename the column by removing suffix source \n",
    "\n",
    "\n",
    "\n",
    "# # # Process insert records\n",
    "max_sk = target_df['inventory_skey_target'].max() if not target_df.empty else 0\n",
    "insert_records = merged_df[merged_df['Action'] == 'INSERT'][source_columns]\n",
    "insert_records['inventory_skey_target'] = range(max_sk + 1, max_sk + 1 + len(insert_records))\n",
    "insert_records.columns = target_columns\n",
    "\n",
    "# # # Process update records\n",
    "update_records_old = merged_df[merged_df['Action'] == 'UPDATE'][target_columns]\n",
    "update_records_new = merged_df[merged_df['Action'] == 'UPDATE'][source_columns]\n",
    "\n",
    "\n",
    "# Depreciate old record\n",
    "update_records_old['endDate_target'] = datetime.now().strftime(DATE_FORMAT)\n",
    "update_records_old['isCurrent_target'] = False\n",
    "update_records_old.columns = target_columns\n",
    "\n",
    "# Update to new record\n",
    "update_records_new['startDate_source'] = datetime.now().strftime(DATE_FORMAT)\n",
    "update_records_new['endDate_source'] = None\n",
    "update_records_new['isCurrent_source'] = True\n",
    "\n",
    "update_records_new.columns = source_columns\n",
    "\n",
    "# # # Process delete records\n",
    "delete_records = merged_df[merged_df['Action'] == 'DELETE'][target_columns]\n",
    "delete_records['endDate_target'] = datetime.now().strftime(DATE_FORMAT)\n",
    "delete_records['isCurrent_target'] = False\n",
    "delete_records.columns = target_columns\n",
    "\n",
    "# Remove all suffix '_source' or '_target' \n",
    "unchanged_records = remove_suffixes(unchanged_records)\n",
    "insert_records = remove_suffixes(insert_records)\n",
    "update_records_old = remove_suffixes(update_records_old)\n",
    "update_records_new = remove_suffixes(update_records_new)\n",
    "delete_records = remove_suffixes(delete_records)\n",
    "\n",
    "\n",
    "\n",
    "# # # Combine all records\n",
    "resultant_df = pd.concat([unchanged_records, insert_records, update_records_old, update_records_new, delete_records])\n",
    "\n",
    "resultant_df.to_csv(\"resultant_df.csv\", index=False)\n",
    "\n",
    "# Load the resultant data back to BigQuery\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)\n",
    "job = client.load_table_from_dataframe(resultant_df, inventory_table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "print(f\"Incremental load with SCD Type 2 logic for inventory data completed successfully. Data saved to {inventory_table_id}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# new_inventory_df = pd.DataFrame({\n",
    "#     'productId': ['prod1', 'prod2', 'prod3'],\n",
    "#     'name': ['Product 1 updated', 'Product 2', 'Product 3'],\n",
    "#     'category': ['Category 1 updated', 'Category 2', 'Category 3'],\n",
    "#     'subCategory': ['Subcategory 1', 'Subcategory 2', 'Subcategory 3'],\n",
    "#     'quantity': [150, 200, 300]\n",
    "# })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
